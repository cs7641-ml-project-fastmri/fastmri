{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmri_common.args import Args\n",
    "from fastmri_common.subsample import MaskFunc\n",
    "from fastmri_data import transforms\n",
    "from fastmri_data.mri_data import SliceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pix2pix_data import create_datset\n",
    "from pix2pix_models import create_model, get_option_setter\n",
    "from pix2pix_util.visualizer import Visualizer\n",
    "from pix2pix_options.train_options import TrainOptions\n",
    "from pix2pix_options.test_options import TestOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomCrop, ToTensor, ToPILImage, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Data Transformer for training U-Net models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_func, resolution, which_challenge, use_seed=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask_func (common.subsample.MaskFunc): A function that can create a mask of\n",
    "                appropriate shape.\n",
    "            resolution (int): Resolution of the image.\n",
    "            which_challenge (str): Either \"singlecoil\" or \"multicoil\" denoting the dataset.\n",
    "            use_seed (bool): If true, this class computes a pseudo random number generator seed\n",
    "                from the filename. This ensures that the same mask is used for all the slices of\n",
    "                a given volume every time.\n",
    "        \"\"\"\n",
    "        if which_challenge not in ('singlecoil', 'multicoil'):\n",
    "            raise ValueError(f'Challenge should either be \"singlecoil\" or \"multicoil\"')\n",
    "        self.mask_func = mask_func\n",
    "        self.resolution = resolution\n",
    "        self.which_challenge = which_challenge\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __call__(self, kspace, target, attrs, fname, slice_):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kspace (numpy.array): Input k-space of shape (num_coils, rows, cols, 2) for multi-coil\n",
    "                data or (rows, cols, 2) for single coil data.\n",
    "            target (numpy.array): Target image\n",
    "            attrs (dict): Acquisition related information stored in the HDF5 object.\n",
    "            fname (str): File name\n",
    "            slice (int): Serial number of the slice.\n",
    "        Returns:\n",
    "            (tuple): tuple containing:\n",
    "                image (torch.Tensor): Zero-filled input image.\n",
    "                target (torch.Tensor): Target image converted to a torch Tensor.\n",
    "                mean (float): Mean value used for normalization.\n",
    "                std (float): Standard deviation value used for normalization.\n",
    "                norm (float): L2 norm of the entire volume.\n",
    "        \"\"\"\n",
    "        kspace = transforms.to_tensor(kspace)\n",
    "        # Apply mask\n",
    "        seed = None if not self.use_seed else tuple(map(ord, fname))\n",
    "        masked_kspace, mask = transforms.apply_mask(kspace, self.mask_func, seed)\n",
    "        # Inverse Fourier Transform to get zero filled solution\n",
    "        image = transforms.ifft2(masked_kspace)\n",
    "        # Crop input image\n",
    "        image = transforms.complex_center_crop(image, (self.resolution, self.resolution))\n",
    "        # Absolute value\n",
    "        image = transforms.complex_abs(image)\n",
    "        # Apply Root-Sum-of-Squares if multicoil data\n",
    "        if self.which_challenge == 'multicoil':\n",
    "            image = transforms.root_sum_of_squares(image)\n",
    "        # Normalize input\n",
    "        image, mean, std = transforms.normalize_instance(image, eps=1e-11)\n",
    "        image = image.clamp(-6, 6)\n",
    "\n",
    "        target = transforms.to_tensor(target)\n",
    "        # Normalize target\n",
    "        target = transforms.normalize(target, mean, std, eps=1e-11)\n",
    "        target = target.clamp(-6, 6)\n",
    "        return image, target, mean, std, attrs['norm'].astype(np.float32)\n",
    "\n",
    "\n",
    "def create_datasets(args):\n",
    "    train_mask = MaskFunc(args.center_fractions, args.accelerations)\n",
    "    dev_mask = MaskFunc(args.center_fractions, args.accelerations)\n",
    "\n",
    "    train_data = SliceData(\n",
    "        root=args.data_path / f'{args.challenge}_train',\n",
    "        transform=DataTransform(train_mask, args.resolution, args.challenge),\n",
    "        sample_rate=args.sample_rate,\n",
    "        challenge=args.challenge\n",
    "    )\n",
    "    dev_data = SliceData(\n",
    "        root=args.data_path / f'{args.challenge}_val',\n",
    "        transform=DataTransform(dev_mask, args.resolution, args.challenge, use_seed=True),\n",
    "        sample_rate=args.sample_rate,\n",
    "        challenge=args.challenge,\n",
    "    )\n",
    "    return dev_data, train_data\n",
    "\n",
    "\n",
    "def create_data_loaders(args):\n",
    "    dev_data, train_data = create_datasets(args)\n",
    "    display_data = [dev_data[i] for i in range(0, len(dev_data), len(dev_data) // 16)]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    display_loader = DataLoader(\n",
    "        dataset=display_data,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, dev_loader, display_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMriToPix2PixTransform(DataTransform):\n",
    "    \n",
    "    def get_crop_positions(self, image, crop_size=256):\n",
    "        r, c = image.shape[-2:]\n",
    "        for i, dim in enumerate(image.shape[-2:]):\n",
    "            if dim < crop_size:\n",
    "                raise ValueError(\"Dimension: {} less than size: {}\".format(i, crop_size))\n",
    "        rp = np.random.randint(r-256)\n",
    "        cp = np.random.randint(c-256)\n",
    "        return rp, cp\n",
    "    \n",
    "    def transform_image(self, image, crop_params, crop_size=256):\n",
    "        r, c = crop_params\n",
    "        return image[...,r:r+crop_size, c:c+crop_size].unsqueeze(0)\n",
    "    \n",
    "    def __init__(self, mask_func, resolution, which_challenge, use_seed=True):\n",
    "        super().__init__(mask_func, resolution, which_challenge, use_seed)\n",
    "    def __call__(self, kspace, target, attrs, fname, slice_):    \n",
    "        image, target, mean, std, norm = super().__call__(\n",
    "            kspace, target, attrs, fname, slice_\n",
    "        )\n",
    "        \n",
    "        crop_params = self.get_crop_positions(image)\n",
    "        \n",
    "        return {\n",
    "            \"A\": self.transform_image(image, crop_params),\n",
    "            \"B\": self.transform_image(target, crop_params),\n",
    "            \"A_paths\": \"DEBUG! Do not use\",\n",
    "            \"B_paths\": \"DEBUG! Do not use\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastMriArgs = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = fastMriArgs.parse_args(\n",
    "    [\n",
    "        \"--challenge\", \"singlecoil\",\n",
    "        \"--data-path\", \"./data/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = MaskFunc(\n",
    "    args.center_fractions,\n",
    "    args.accelerations\n",
    ")\n",
    "dev_mask = MaskFunc(\n",
    "    args.center_fractions,\n",
    "    args.accelerations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SliceData(\n",
    "    root=args.data_path / f'{args.challenge}_train',\n",
    "    transform=FastMriToPix2PixTransform(\n",
    "        train_mask, args.resolution, args.challenge, use_seed=True\n",
    "    ),\n",
    "    sample_rate=args.sample_rate,\n",
    "    challenge=args.challenge,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = SliceData(\n",
    "    root=args.data_path / f'{args.challenge}_val',\n",
    "    transform=FastMriToPix2PixTransform(\n",
    "        dev_mask, args.resolution, args.challenge, use_seed=True\n",
    "    ),\n",
    "    sample_rate=args.sample_rate,\n",
    "    challenge=args.challenge,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_options = TrainOptions()\n",
    "test_options = TestOptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# !python train.py --dataroot ./datasets/facades --name facades_pix2pix_mason --model pix2pix --direction BtoA\n",
    "\n",
    "!python train.py --dataroot ./datasets/facades --name facades_label2photo --model pix2pix --batch_size 16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = [\n",
    "    \"--dataroot\", \"./\",\n",
    "    \"--name\", \"pixel_low_lr\",\n",
    "    \"--model\", \"pix2pix\",\n",
    "    \"--input_nc\", \"1\",\n",
    "    \"--output_nc\", \"1\",\n",
    "    \"--no_flip\",\n",
    "    \"--display_id\", \"-1\",\n",
    "    \"--isTrain\", \"True\",\n",
    "    \"--gpu_ids\", \"0\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--netG\", \"unet_256\",\n",
    "    \"--netD\", \"pixel\",\n",
    "    \"--lr_policy\", \"cosine\",\n",
    "    \"--lr_g\", '0.00001',\n",
    "    \"--lr_d\", '0.000005',\n",
    "    \"--continue_train\",\n",
    "    \"--epoch\", \"1\",\n",
    "#     \"--gan_mode\", \"wgangp\",\n",
    "    \"--verbose\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_args = [\n",
    "    \"--dataroot\", \"./\",\n",
    "    \"--name\", \"val\",\n",
    "    \"--model\", \"pix2pix\",\n",
    "    \"--input_nc\", \"1\",\n",
    "    \"--output_nc\", \"1\",\n",
    "    \"--no_flip\",\n",
    "    \"--isTrain\", \"False\",\n",
    "    \"--gpu_ids\", \"0\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--verbose\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = argparse.ArgumentParser()\n",
    "test_opts = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = train_options.initialize(train_opts)\n",
    "test_opts = test_options.initialize(test_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--isTrain'], dest='isTrain', nargs=None, const=None, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_opts.add_argument(\"--isTrain\", default=True)\n",
    "test_opts.add_argument('--isTrain', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = get_option_setter(\"pix2pix\")(train_opts)\n",
    "test_opts = get_option_setter(\"pix2pix\")(test_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = train_opts.parse_args(train_args)\n",
    "test_opts = test_opts.parse_args(test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=train_opts.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=test_opts.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n"
     ]
    }
   ],
   "source": [
    "model = create_model(train_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurrah!\n",
      "loading the model from ./checkpoints/pixel_low_lr/1_net_G.pth\n",
      "loading the model from ./checkpoints/pixel_low_lr/1_net_D.pth\n",
      "---------- Networks initialized -------------\n",
      "UnetGenerator(\n",
      "  (model): UnetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (3): UnetSkipConnectionBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (3): UnetSkipConnectionBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (3): UnetSkipConnectionBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (2): ReLU(inplace)\n",
      "                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                )\n",
      "                              )\n",
      "                              (4): ReLU(inplace)\n",
      "                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (7): Dropout(p=0.5)\n",
      "                            )\n",
      "                          )\n",
      "                          (4): ReLU(inplace)\n",
      "                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (7): Dropout(p=0.5)\n",
      "                        )\n",
      "                      )\n",
      "                      (4): ReLU(inplace)\n",
      "                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (7): Dropout(p=0.5)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): ReLU(inplace)\n",
      "                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "              )\n",
      "              (4): ReLU(inplace)\n",
      "              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (4): ReLU(inplace)\n",
      "          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ReLU(inplace)\n",
      "      (3): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (4): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[Network G] Total number of parameters : 54.408 M\n",
      "PixelDiscriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "[Network D] Total number of parameters : 0.009 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.setup(train_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"Discriminator_LRG_1e-5_LRD_5e-6_D-SGD_pixel_low_lr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_loss_scalars(writer, losses_dict, step):\n",
    "    for key in losses_dict.keys():\n",
    "        writer.add_scalar(key, losses_dict[key], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2234bb7e3e774bfe945d73b6d68050e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682b68f6b5b43458cdecba74b15a2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch_ in tqdm(range(num_epochs)):\n",
    "    epoch = epoch_+1\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        model.set_input(batch)\n",
    "        model.optimize_parameters()\n",
    "        losses = model.get_current_losses()\n",
    "        write_loss_scalars(writer, losses, epoch*dataset_size+i)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader)):\n",
    "            outputs = model.netG(batch['A'].to(torch.device(\"cuda:0\")))\n",
    "            writer.add_image(f\"generated_\"+comment, make_grid(outputs.cpu()), epoch)\n",
    "            writer.add_image(f\"subsampled_\"+comment, make_grid(batch['A']), epoch)\n",
    "            writer.add_image(f\"target_\"+comment, make_grid(batch['B']), epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_filename = \"1_net_G.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_path = model.save_dir+\"/\"+save_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model.netG.cpu().state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 = create_model(train_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1.load_networks(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
